See the "Physics Considerations" section of [[../../DESIGN-II.org]] for background
on some of the high-level decisions here. Here, we focus on lower-level things.

* Performance Numbers

Goals set forth in the high-level design:

- 10ms update cycle takes at most 1ms
- Handle up to 10'000 independent objects

Target minimum hardware:

- 2GHz processor (with comparable instruction/cycle rate of the Broadwell era)
- 1GB/sec memory write bandwidth (no cache)
- 32kB L1 cache
- 256kB L2 cache
- 2MB L3 cache
- Assume
  https://mechanical-sympathy.blogspot.ca/2013/02/cpu-cache-flushing-fallacy.html
  is representative for memory access latencies:
  - 3 cycles for L1 hit (~1ns)
  - 12 cycles for L2 hit (~6ns)
  - 38 cycles for L3 hit (~19ns)
  - 130 cycles for hitting RAM (~65ns)

10'000 objects in 1ms means 10'000'000 object updates per second, or 100ns per
object update. At the minimum required processor speed, this means we get 200
clock cycles average per object update. Note that this means that a DRAM hit is
over half our budget for average update time. This means we need to ensure the
majority of object data can fit inside the L3 cache and doesn't conflict with
itself.

* High-level design

All coordinates are expressed as integers rather than floats, which gives space
a uniform quantisation and ensures reproduciblity.

Spatial coordinates are signed 32-bit integers. A screen-width at default zoom
is 65536 units across. Negative absolute coordinates are not normally used;
signedness is used for expressing relative coordinates. Screen coordinates
follow 2D drawing conventions: +X is right, +Y is down.

Most physics however take place in hexagonal coordinates (as defined in
`coords.rs`).

Velocity is simply a pair of signed integers indicating delta position per
tick. This quantises velocity to 100 units/sec, or about one pixel per 3
seconds on a 1920-wide monitor. We only need 16 bits for each velocity
component, since 50 screens/sec (v=32768) is far higher than any speed of light
we'd care to set anyway.

Rotation is expressed as a signed 16-bit integer, with 0 being 0 radians and
-32768 being -pi radians. Wrapping arithmetic means that angles are always
conveniently normalised to (pi,-pi] radians (which was a big pain to do in
Abendstern). Since the coordinate system is left-handed, positive angle is
clockwise.

One obvious thing about objects in space is that a few behaviours are common to
everything, and many objects have _only_ these behaviours in the absence of
interactions with other objects:

- Position
- Rotation
- Velocity
- Rotational velocity
- Size (radius of bounding circle)
- Friction

If we split objects into these common properties plus extended properties, the
common properties can be stored uniformly and contiguously. The common
properties would then have a pointer of sorts to the extended data.

To minimise accesses to the extended data, we can also have a concept of an
object "sleeping", expressed as a number of ticks until it either needs to do
something that modifies its extended state or interacts with the static
environment.

To quicken collision detection, we may use the same system Abendstern did,
wherein we keep the objects (in this case, just the common properties) sorted
ascending by the rightmost A coordinate of their bounding circle.

* Common Data Layout

Goal: Cram all the common data into 2 128-bit SSE registers.

Something like the below (note that the real thing is slightly different):

xmm0[  0.. 31] i32 A coordinate + rounded-radius
xmm0[ 32.. 63] i32 B coordinate
xmm0[ 64.. 79] i16 Theta
xmm0[ 80.. 95] u8 Rounded-radius / 256
xmm0[ 96..103] u8 Collision group
xmm0[104..111] u8 Object type
xmm0[112..128] u16 Extended data pointer (if applicable)
xmm1[  0.. 15] i16 A velocity
xmm1[ 16.. 23] u8 A friction
xmm1[ 24.. 31] i8 A acceleration
xmm1[ 32.. 47] i16 B velocity
xmm1[ 48.. 55] u8 B friction
xmm1[ 56.. 63] i8 B acceleration
xmm1[ 64.. 79] i16 Theta velocity * 4
xmm1[ 80.. 87] u8 Theta friction
xmm1[ 88.. 95] i8 Theta acceleration
xmm1[ 96.. 103] u8 Ticks until wake-up
xmm1[ 104..127] ???

Using 16-bit velocity gives us enough space to also include passive
acceleration (as signed bytes), which allows encoding relatively complex
movements without wakeup and only using processor registers.

Note that velocities are laid out parallel to the coordinates so they can be
added to the position without extracting them first (after bitmasking the other
data away).

The way the A coordinate is stored means that sorting values by treating xmm0
as an i32x4 is sufficient to achieve the correct ordering.

Data is also laid out such that collision detection only cares about xmm0 while
sweeping.

"Rounded radius" is the physical radius of the object (relative to the nominal
A coordinate) rounded up to the next multiple of 256.

Not all object types need extended data; those which don't can store extra
information in the extended data pointer field.

Since each object has 32 bytes of common data, the common data array for 10'000
objects takes 320'000 bytes, which is unfortunately larger than our L2 cache,
but fits with much room to spare in the L3 cache. Since the CPU can flush
writes to lower levels asynchronously provided we don't outrun that process
(which we won't), we only need to care about the L3 cost of reads. A passive
object thus burns half a cache line per update, so we lose around 10ns to
memory latency per passive update. If the SSE prefetch instruction is
available, we can prefetch upcoming objects to L1 cache, eliminating this
price.

* Snapshotting and Data Placement

We need to periodically take a snapshot of the state of the world so that we
can roll back to it if a new event comes in after the time it is supposed to
apply.

There's two ways to approach this:

- When a snapshot is taken, allocate new space for the data and copy all state
  into it. Updating the world simply overwrites data in-place.

- Take a write-once (per frame) approach to updating. Snapshots simply retain a
  reference to the root of state at that frame; updates allocate new space and
  write data there.

The first approach seems simpler, and given that we're cramming everything into
at most a few megabytes of state, a few snapshots per second would be fairly
cheap. However, this approach also has some unexpected downsides:

- Creation and destruction of objects need to create or remove holes in the
  sorted array. This means on average half of the common state would be
  rewritten anyway.

- Objects which change state in response to other objects see an intermediate
  state of the world, so a canonical order of updating objects must be defined.

- Rendering and updating cannot happen simultaneously since rendering needs the
  state of the world to be steady (though not necessarily consistent).

Therefore we actually use the second approach. Each frame, the common data is
completely rewritten. Objects which are created or destroyed as part of the
update process do so when they are/should be very near the end of the array.
Objects that observe other objects instead see a consistent state of the world
of the _previous_ frame. Rendering can be asynchronous wrt updating.

Since we have a no-overwrite policy, objects which change their extended data
must allocate new space, write their state there, then update the extended data
pointer. To make this efficient, and to reduce pointer size to 16 bits, this
happens within a 1MB heap with 16-byte allocation granularity. Allocation
simply increments the free pointer as needed. It is trivial to tell whether an
allocation is new for this frame by comparing the allocation address with what
the free pointer was at the beginning of the frame. When the heap gets too
full, we make a new one and garbage collect by copying all live objects over.

In this system, making snapshots is essentially free. We still don't want to
do snapshots every frame or something though, since each snapshot retains a
unique common data array, which is 320kB in the worst case.

* Collision Detection

Since we don't want to pay extra memory/cache bandwidth that would be needed by
the traditional two-pass update-then-collide system, we instead perform
collision detection inline with updating. After the general updating of an
object, we "slide" it into place in the common data array, then scan backwards
for objects whose A bounding box (A-2*radius..A) overlaps with the object in
question. Since we keep the array sorted by the upper A bound, we only need to
look at a handful of objects here.

The system has two types of physical objects: point particles, with radius = 0,
and composites, which have their bounds defined by whether tiles on a regular
hexagonal grid are populated. All composites have hexagons of the same size.
Point particles cannot collide with other point particles, so when scanning for
collidees for a point particle, we skip other point particles.

Objects with the same non-zero collision group are skipped. This is not so much
an optimisation as it is a mechanism to prevent projectiles fired by a player
from immediately colliding with their ship.

Objects overlapping on the A axis are then checked whether their B axis bounds
overlap; those that do not are skipped.

At this point, we need to do precise collision detection. There's two cases:

- Point particle and composite. We convert the point particle's position into
  hexagonal coordinates relative to the composite, then directly check what (if
  anything) is in that position of the composite's grid.

- Two composites. We assume that two composites will only touch at exactly one
  point; this can only false if two composites are at just the right rotation.
  We iterate over the cells of the smaller composite and project each to the
  grid of the larger one. If we find a pair of overlapping cells, we consider
  it a collision, and proceed with the pair with the closest coordinates.

For both cases, we need to consider the case where the relative speed of the
two objects is greater than half a cell width per frame; the naïve approach
opens the door for missing collisions entirely or detecting them on the wrong
side of an object.

We deal with this by instead treating each point of the smaller object as a
line segment, obtained by assuming all velocities (including rotation) are
exactly representative for how each object moved this frame to determine the
relative positions of the points in question at the start and end of the frame
and assuming the points moved in a straight line between those positions.
Determining the point of collision (if any) then requires simply evaluating the
resulting linear equation a few times. Note that the larger object is always a
composite, so we can still rapidly determine the candidate collision points by
tracing the line segment across the hex grid.

Another problem is that we only detect collisions after objects are
overlapping. Some systems deal with this by shunting the object(s) so this is
not the case. However, repositioning the objects requires re-sorting the object
array, and can cause overlap with other objects. The problems this causes can
be seen in games like Skyrim, particularly when objects are stacked.

Instead, we simply allow the objects to continue overlapping, and additionally
define a collision to not happen if the dot product between the relative
positions of the points in question is not positive at the time of presumed
impact. Since objects undergoing collision have their velocities updated to
move away from each other (at least at those points), the next frame will then
not consider the overlapping objects to be colliding and they will drift apart
naturally.

To support composites which need to add new cells dynamically, there is also a
"virtual cell" concept. A virtual cell exists for only one frame; if it
collides with something, the virtual cell is notified (so it can be removed the
next frame) but no other collision dynamics occur. After that one frame of
existence, a real cell can be placed there, as any new collision is
functionally equivalent to what would have happened if the cell already existed
there for some time.

Unlike in Abendstern, objects are not allowed to change their properties
related to collision detection (including being destroyed or creating new
objects) in response to colliding with something. Instead, they must store this
state and apply this effect in the next frame.

** Alternate collision detection option — Approximate Hilbert-like tree

Sorted scanning is still O(n²) with respect to the total number of objects,
even though it has a much lower constant factor than naïve sweeping. The main
problem is that we have no way to eliminate candidates which happen to be near
on one coordinate but very far on another.

The core idea here is to make each object a node in an R-tree in encounter
order, using implicit conenctions as used with a binary heap in an array. The
overhead for each object is constant and branchless: retrieve the bounding
boxes of the children, expand them to include each other and the object itself,
and write to the tree array.

In order to keep objects encountered near each other actuall near each other,
the object array is occasionally re-sorted by Hilbert coordinate (say every 100
frames or so); between sorts, objects are allowed to drift out of order, on the
assumption that they will still be near each other.

Whereas sweeping needs to check each object against every object already
processed, tree-based detection would instead test objects against smaller
objects, only falling back to order as a tie-breaker. This means that point
particles would not need to be checked at all since the only things they can
collide with are strictly larger than them.

*** Advantages over sweeping

- No variably-timed sorting step. Simply reads two words, does branch-free
  operations on them, and writes one word.

- No need to check point particles for collisions, substantially reducing the
  number of tree walks.

- Objects far apart on the B axis are filtered off from each other.

- The collision detection step can be parallelised.

*** Disadvangates to sweeping

- Adds 16 bytes of memory bandwidth per object which can't be coalesced away.

- Adds 16 bytes of memory per object (160kB for 10k objects) which must contend
  for the caches.

- Performs random memory accesses as the tree is walked.

- Collision detection step is "off-line" and cannot start until the object
  update step is complete.

- No fast "best case"; every object will at least walk the tree enough to find
  itself, an average of `log2(n)-1`, or 13 tests for 10k objects.

- High-speed objects can wreck the quality of the tree.

- More complex.

*** Considerations

Cache concerns can probably be dealt with by explicit prefetching.

A player using a rapid-fire weapon parallel to the B axis would quickly force
the sweeping method to do far more than 13 tests per object along that column.

Being able to skip point particles entirely eliminates a branch that the
sweeping algorithm needs in an inner loop, and also effectively coalesces tree
walks into larger objects.

Not clear how to deal with fast-moving objects. Two objects in leaf positions
at maximum speed in opposite directions will quickly make that entire branch of
the tree huge. Tweaking the sort order to try to put volatile objects into
upper branches isn't really viable since objects are created and destroyed
rapidly, constantly shifting everything around. Just sorting more frequently
could be viable. Re-sorting could actually take place independently of
rewind/fast-forward since it is not required for physical accuracy, so we could
re-sort much more frequently in most cases.

Sorting 10k objects already sorted by transforming 24 bit (A,B) to Hilbert
values takes 680μs on my AMD64 system, so probably around 3ms on the target
minimum. If the number of dimensional bits is reduced to 8, this speeds up to
154μs. My ARM chromebook takes 5ms and 1.6ms, respectively.

** Collision detection option — On-line approximate Hilbert-like tree

The above can be made on-line by checking each object for collisions with those
that come before it. Since the tree is built in encounter order, checking each
object for other objects encountered before it is sufficient.

Note that in most cases there are two trees for each object: the one rooted at
the object, and the one that would be the object's prior sibling once the
object has a parent.

*** Advantages over off-line algorithm

- It is on-line and so can be performed concurrently to the object update step.

- Average tree depth for each object is 12.3 for 10k objects, vs 14.

- Objects do not need to traverse the tree to ultimately just find themselves;
  since they start at their own node, the branches are immediately interesting
  and many objects will not traverse the entire tree.

*** Disadvantages to off-line algorithm

- Every object fully participates in collision detection, resulting in more
  tree walks.

- Need an extra branch for the point-particle-point-particle case (but which is
  only relevant if two point particles reach the exact same spot).

*** Considerations

The simplest performance aspect is that the off-line algorithm does better when
most objects are point particles, whereas the on-line algorithm isn't affected
as obviously. With 10k objects, the off-line algorithm does on average 13
comparisons per composite to locate that composite's own node, so if 50% of
objects are composites, we pay 65000 comparisons. The on-line algorithm, if it
always descended every level of the tree, would do 123000 comparisons.

However, that doesn't take into account the fact that the off-line algorithm
_always_ needs to traverse the tree to a particular composite, but the on-line
algorithm does not. If we assume that on average we only need to down half the
tree to prune it, we only need around 62000 comparisons.
